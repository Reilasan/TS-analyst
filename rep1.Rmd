---
title: "multiple regression"
output: html_document
---

<center><h1>Многофакторные регрессионные уравнения</h1></center>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(caTools)
library(DescTools)
library(dplyr)
library(ggplot2)
library(lmtest)
library(glmnet)
library(nortest)
library(tidyverse)
library(sm)
library(knitr)
library(htmltools)
library(broom)
library(car)
library(data.table)
library(tidyverse)
library(foreach)
library(ggcorrplot)
library(gridExtra)
library(grid)
library(ggExtra)
library(Metrics)
library(e1071)
library(pedometrics)
library(scales)
library(psych)
library(corrgram)
```

<ol><font size="5">Ход работы:</font>
<li> <font size="3">Загрузка данных:</font></li>
<br>price - это цена бриллианта в долларах США. 
<br>carat - это вес бриллианта. 
<br>cut - это качество огранки (удовлетворительное, хорошее, очень хорошее, премиум, идеальное). 
<br>color, от J (худший) до D (лучший). 
измерение чистоты алмаза (I1 (наихудший), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (лучший)) <br>X - длина в мм 
<br>Y - ширина в мм 
<br>Z - глубина в мм 
<br>depth - общая глубина в процентах 
<br>table - ширина вершины алмаза относительно самой широкой точки
```{r}
z <- read.csv(file = "diamonds.csv", header = TRUE, sep = ",", dec =".")
str(z)
```
**[NA значения]**
```{r}
sapply(z, function(x) sum(is.na(x)))
```

<br>
<li> <font size="3">Подготовка данных:</font> </li>
```{r message=FALSE, warning=FALSE}
vars <- names(z) %in% c("x", "y", "z")
data <- z[vars]
vol<-data$z*data$x*data$y
myvars <- names(z) %in% c("carat", "depth","table", "price")
mydata <- z[myvars]
mydata$volume<-vol
set.seed(27)
split0 <- sample.split(mydata, SplitRatio = 0.3)
mydata <- subset(mydata, split0 == TRUE)
litdata <- subset(mydata, split0 == TRUE)
str(mydata)
corr.test(mydata)
corrgram(mydata, order=TRUE)
scatterplotMatrix(litdata, spread=FALSE)
```

<br>
<li> <font size="3">Для оценки качества регресионной модели разделим выборку на тестовую и обучающую:</font> </li>

```{r}
set.seed(1827)
split <- sample.split(mydata$price, SplitRatio = 0.75)
train <- subset(mydata, split == TRUE)
test <- subset(mydata, split == FALSE)
train_y <- train$price
train_x <- train%>%select(-price)%>%data.matrix()
test_x <- test%>%select(-price)%>%data.matrix()
```
<br>
<li> <font size="3">Построим модель линейной регрессии:</font></li>
```{r}
m_lm <- lm(price~carat + depth + table + volume,data = train)
summary(m_lm)
glance(m_lm)
```

```{r}
par(mfrow=c(2,2))
plot(m_lm)
```

<br>
<li> <font size="3">Для анализа мультиколлениарности вычислим значения коэффициентов увеличения дисперсии VIF:</font> </li>
```{r}
VIF(m_lm)
```
>Поскольку коэфициенты больше 10, то мультиколлинеарность присутсвует.

<li> <font size="3">Проверка выполнения условий Гаусса-Маркова:</font> </li><br>
Проверка несмещенности:
<blockquote>
H0: Математическое ожидание остатков равно нулю<br>
H1: Математическое ожидание остатков не равно нулю
</blockquote>
  Вычислим t-расчётное и t-табличное
```{r}
a <- mean(m_lm$residuals)
b<-sd(m_lm$residuals, na.rm = FALSE)
n <- sqrt(13338)
tm <- a/b*n
str(tm)
qt(0.975,13337)

```
>t-расчётное меньше < t-табличного - нулевая гипотеза о равенстве математического ожидания остатков нулю подтверждается.

Проверка инвариантности дисперсии:
<blockquote>
H0: Отсутсвие гетероскедастичнсти<br>
H1: Наличие гетероскедастичности
</blockquote>

Проведем тест Бройша-Пагана
```{r}
bptest(m_lm)
```
>Так как значение p-value меньше 0.05, нулевая гипотеза о гомоскедастичности остатков не подтверждается.


Проверка отсутствия автокорреляции:
<blockquote>
H0: ρ=0 (т.е. автокорреляция остатков отсутствует)<br>
H1: ρ>0 или ρ<0 (наличие положительной или отрицательной автокорреляции остатков)
</blockquote>
Проведем тесты Бройша- Годфри и Дарбина-Уотсона:
```{r}
bgtest(m_lm)
dwtest(m_lm )
```
>Поскольку значение p-value меньше 0.05 нулевая гипотеза об отсутствии автокорреляции остатков не подтверждается.

Проверка согласовнности остатков регрессии с нормальным законом:<br>
Рассмотрим графические тесты:
```{r}
qqnorm(m_lm$residuals)
```

```{r}
Z<- m_lm$residuals
hist(Z)
```

```{r}
sm.density(m_lm$residuals, model = "Normal",xlab = "Resudual", ylab = "Функция плотности распределения")
```

Рассмотрим параметрические тесты:<br>
Тест Лиллиефорса:
<blockquote>
Н0: распределение остатков не отлично от нормального<br>
Н1: распределение остатков отлично от нормального
</blockquote>
```{r}
lillie.test(m_lm$residuals)
```
>Так как значение p-value меньше 0.05, нулевая гипотеза о согласии распределения остатков с нормальным законом распределения не принимается.

<li> <font size="3">Построим прогноз по полученной модели на тестовой выборке</font> </li><br>
```{r}
test$predict <- predict(m_lm, test)
str(test)
```



```{r}
ggplot(test, aes(x = price, y = predict)) + geom_point() + geom_abline()
```
<li> <font size="3">Оценим параметры с помощью метода Ридж:</font> </li><br>
```{r}
set.seed(1877)
lambdas <- seq(0, 1000, by = 2)
cv_ridge <- cv.glmnet(train_x, train_y, alpha = 0, lambda = lambdas)
plot(cv_ridge)
```

Определим подходящие значения λ:
```{r}
cv_ridge$lambda.min
cv_ridge$lambda.1se
```
Построим модель линейной регрессии с помощью метода Ридж:
```{r}
m_ridge <- glmnet(train_x, train_y, alpha = 0, lambda = cv_ridge$lambda.min)
coef(m_ridge)
m_ridge_1se <- glmnet(train_x, train_y, alpha = 0, lambda = cv_ridge$lambda.1se)
coef(m_ridge_1se)
```
Построим прогноз для полученной модели на тестовой выборке:
```{r}
test$ridge <- predict(m_ridge, s = cv_ridge$lambda.min, newx = test_x)
str(test)
```

<li> <font size="3">Оценим параметры модели с помощью метода Lasso</font> </li><br>
```{r}
set.seed(1886)
lambdas <- seq(0, 600, by = 1)
cv_lasso <- cv.glmnet(train_x, train_y, alpha = 1, lambda = lambdas)
plot(cv_lasso)
```

Определим подходящие значения λ:
```{r}
cv_lasso$lambda.min
cv_lasso$lambda.1se
```
Построим модель линейной регрессии с помощью метода Lasso:
```{r}
m_lasso <- glmnet(train_x, train_y, alpha = 1, lambda = cv_lasso$lambda.min)
coef(m_lasso)
m_lasso_1se <- glmnet(train_x, train_y, alpha = 1, lambda = cv_lasso$lambda.1se)
coef(m_lasso_1se)
```
Построим прогноз по полученной модели на тестовой выборке:
```{r}
test$lasso <- predict(m_lasso, s = cv_lasso$lambda.min, newx = test_x)
str(test)
```

<li> <font size="3">Рассчитаем метрики MAE (средняя абсолютная ошибка), RMSE (квадратный корень из среднеквадратичной ошибки), MAPE (средняя абсолютная ошибка в процентах) для моделей, полученных с использованием различных методов для оценки параметров.</font> </li><br>
```{r}
MAPE(x = test$predict, ref = test$price)
RMSE(x = test$predict, ref = test$price)
MAE(x = test$predict, ref = test$price)
MAPE(x = test$ridge, ref = test$price)
RMSE(x = test$ridge, ref = test$price)
MAE(x = test$ridge, ref = test$price)
MAPE(x = test$lasso, ref = test$price)
RMSE(x = test$lasso, ref = test$price)
MAE(x = test$lasso, ref = test$price)

```

Результаты сравнения метрик<br>


| Met  | MAPE | RMSE |  MAE |
| --- |---| ---| --- |
| МНК      | 0.3917599 | 1515.276 | 983.3468 |
| Ридж      | 0.3892067   | 1514.51 |  981.3798 |
| Лассо | 0.3919691   |  1515.023 | 983.3372 |

Тест Рамсея:
<blockquote>
Н0: Спецификация модели является правильной<br>
Н1: Спецификация модели является неправильной
</blockquote>

```{r}
resettest(m_lm)
```
</ol>

